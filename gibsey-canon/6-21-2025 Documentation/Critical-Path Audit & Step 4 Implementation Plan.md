Great — I’ll now audit the current `main` branch of your repo to verify the implementation quality of Steps 1–3 (canonical data cut, tokenizer integration, retrieval API), note any issues to fix, and then create a complete plan to implement Step 4: Single Agent Voice using Jacklyn Variance as the first persona. I’ll factor in your support for Ollama, OpenAI, and Claude, and choose the cleanest integration point for your current design.

# Gibsey Project Critical-Path Audit & Step 4 Implementation Plan

## Audit of Steps 1–3

**Step 1 – Canonical Data Cut:** The corpus freeze has been completed successfully. The repository contains **710 Markdown files** under `gibsey-canon/corpus/pages/`, each representing a story page. A metadata JSON (`pages.json`) and a master index (`index.json`) have been generated to map page IDs and character symbols, confirming that the original monolithic source has been flattened and all **symbol definitions are integrated** into the new structure (the CI validation checks that every `symbolId` in pages appears in the index and has a corresponding character folder). The Git tag **`corpus-v1.0`** is present to mark the frozen 710-page corpus version. Additionally, a **CI workflow** is in place to lint and validate corpus changes: on every push/PR affecting `gibsey-canon/**`, the workflow runs a script to ensure 710 pages are present, metadata matches, UTF-8 encoding is valid, and each character has the required files. This means any discrepancy in page count or character assets will cause CI to fail, enforcing the integrity of the canonical data cut.

**Step 2 – House Tokenizer:** A custom Byte-Pair Encoding tokenizer has been trained on the full corpus with a vocabulary of ~**31.5k tokens**, which **includes all 16 character glyph tokens and the 4 QDPI role tokens** as special entries. The tokenizer model (`gibsey_bpe.model` with its vocab and JSON export) is stored in the `tokenizer/` directory. The **backend integration** is complete: the Python backend loads this SentencePiece model at startup and recognizes the special tokens (e.g. `<X_READ>, <Y_INDEX>, <A_ASK>, <Z_RECEIVE>`, plus character IDs like `jacklyn-variance`) so they are never split. The tokenizer service provides utility methods for token counting, encoding/decoding, chunking, and TNA cost calculation, which are used both in the Python embedding pipeline and can be accessed via the Bun/TypeScript frontend. On the frontend, a corresponding `tokenizerService.ts` uses the JSON export to enable token counts and special token detection in the UI (ensuring consistency with the backend). A **CLI tool** is available for developers to visualize tokenization: running `npm run token-viz "Your text"` invokes `scripts/token_viz.py`, which prints the token breakdown, highlights any special tokens present, and computes the TNA credit cost. In summary, the tokenizer step is fully implemented and integrated – the custom model files are in place, and both backend and frontend code have been updated to use the new tokenizer for consistent token counts and special token handling.

**Step 3 – Retrieval API:** The groundwork for retrieval is partially in place. An **embedding seeding script** (`scripts/seed_embeddings.py`) reads all 710 pages from the frozen corpus and inserts their embeddings and metadata into the Cassandra database. This confirms that vector representations of each page (along with page IDs, titles, symbols, and token counts) are stored in Cassandra for semantic search. The seeding process uses either OpenAI or a local SentenceTransformer model to generate embeddings and loops through every Markdown file, so all pages are indexed (the script logs a confirmation of processing all pages, expecting 710/710 pages). However, the **public-facing Retrieval API endpoints** (`POST /read/<page_id>` to fetch a page, and `POST /index?q=…` to query the index) do not appear to be fully wired up in the main branch. There is no explicit handler in the Bun server or FastAPI code for these endpoints visible in the repository. In fact, the front-end chat interface is currently using stubbed responses for Jacklyn’s answers instead of calling a real search/answer service – for example, the React code generates a fake “Response Protocol” message based on hardcoded patterns and delays, rather than fetching results from a backend. This suggests that while the data layer (embeddings in Cassandra) is ready, the **query-time integration** is not yet finalized – the logic to take a user query, perform a vector search in Cassandra, and return the relevant page snippets is only partially implemented (likely in a backend service class, but not exposed via an API route). Additionally, we did not find any **unit tests** targeting the retrieval functionality – e.g. no tests simulating calls to `/index` or verifying that the correct pages are returned. This is a gap in test coverage and indicates that the retrieval endpoints might still be in development or awaiting integration.

_Findings:_ Steps 1 and 2 have been **implemented correctly and comprehensively** – the corpus is properly frozen/versioned with CI enforcement, and the custom tokenizer is trained and integrated across the stack. For Step 3, the background work of indexing the corpus is done (embeddings exist in the DB), but there are **signs of incomplete integration**: the actual API routes and automated tests for retrieval are missing or not fully hooked up. To close Step 3, the team should expose the Cassandra-backed search and read functionality via the intended endpoints and add tests to validate search results and page fetches. Ensuring the front-end or CLI can query this API will set the stage for the next phase.

## Implementation Plan – Step 4: Single Agent Voice (Receive)

Step 4 aims to implement the **“Receive” stage** of the QDPI loop, enabling a single AI agent (Jacklyn Variance) to accept a user’s query along with relevant context and produce a narrative response. We will leverage the components from Steps 1–3 (the canonical corpus, tokenizer, and retrieval system) to build this feature. The goals are to use **Jacklyn Variance’s persona** as the voice, employ a **selectable LLM backend** (preferring the local Ollama model, with cloud API fallbacks), incorporate the `<X_READ>` **context** in prompts, and deliver an answer that includes the `<Z_RECEIVE>` header token in proper format. Below is a detailed plan addressing design, integration, testing, and logging:

**1. LLM Backend Wrapper (Ollama + Fallbacks):** Develop a minimal **LLM service module** in the backend that abstracts the details of calling different language model backends. This service will first attempt to generate responses using **Ollama**, which is expected to run a local LLM (e.g. a LLaMA-2 variant) via its HTTP API or CLI. If the local model is unavailable or produces an error/time-out, the service will **fall back to an external API** (such as OpenAI’s GPT-4 or Anthropic’s Claude) using the appropriate SDK and API keys. This wrapper should expose a simple method (e.g. `generate_response(prompt:str) -> str`) that handles the retry logic.

- _Implementation:_ Use environment flags or config to determine which backend is primary. For Ollama, the service might issue an HTTP POST to `http://localhost:11434/api/generate` (if Ollama’s daemon is running) with the chosen model and prompt. Capture the response stream or final output. If that call fails (catch exceptions or non-200 responses), log the failure and then formulate a request to the OpenAI API (using `openai.ChatCompletion` with the prompt) or to Claude’s API. The wrapper normalizes the output from either source into a plain text string (since ultimately we just need the model’s answer text). This design isolates model selection in one place.
    
- _Logging:_ When a request is made, log which model backend is being used. If a fallback occurs, log a warning like “Ollama model unavailable, switching to OpenAI API for this request.” Include timing information (how long each call took) for performance monitoring. This helps in debugging and in evaluating if the local model is reliable or if fallback is happening frequently.
    
- _Configuration:_ Ensure model identifiers and API keys are configurable (e.g. via environment variables for OpenAI/Claude keys, and a config value for the Ollama model name to use). This allows easy switching of the primary model or using different models via config changes without code modifications.
    

**2. Persona and Prompt Composition:** Jacklyn Variance’s **narrative persona** must be infused into the prompt so that the LLM’s output stays in character. We will craft a **prompt template** that includes: the retrieved context annotated with `<X_READ>`, an indication of the question (possibly with `<A_ASK>`), and instructions or examples reflecting Jacklyn’s voice. The tokenizer already recognizes these special tokens, so they can be included literally in the prompt.

- _Jacklyn’s Voice:_ Jacklyn is essentially a _surveillance analyst_ with a formal, observational tone. Her reports follow the **“D.A.D.D.Y.S-H.A.R.D”** format (as seen in the stubbed responses) – including a report number, a **Subject line**, analytical paragraphs, and a signature `"—JV"`. To achieve this, use a **system message** (in OpenAI terms) or a preamble in the prompt telling the model to respond as Jacklyn. For example: _“You are **Jacklyn Variance**, a surveillance analyst AI speaking in a formal report style. Analyze the user’s query as if it were an observation, and respond in the format of a D.A.D.D.Y.S-H.A.R.D report. Begin your answer with `<Z_RECEIVE>` followed by a brief report title, then `Subject: ...`, then the analysis, ending with ‘—JV’.”_ This instruction sets the stage for the desired output format. We can even show a **few-shot example** in the prompt if needed (a short dummy Q&A in Jacklyn’s style) to reinforce the format. The repository’s existing content provides guidance – for instance, the UI currently seeds a greeting in Jacklyn’s style, and expects responses similarly structured (numbered reports, “Subject:” lines, etc.). We will mirror that style.
    
- _Including `<X_READ>` Context:_ The relevant corpus information retrieved in Step 3 will be inserted into the prompt, preceded by the `<X_READ>` token. For example, if the user asks about a particular event or character, the top N relevant page snippets (or summaries) will be concatenated as the “Read” context. The prompt to the model might look like:
    
    ```text
    <X_READ>
    [Excerpt 1 from relevant page ...]
    [Excerpt 2 from another page ...]
    …
    <A_ASK> {user_query}
    <Z_RECEIVE>
    ```
    
    In a ChatML format (for OpenAI), this could be structured as a system message containing the context and instructions, followed by a user message with the actual question. The key is that the model sees the `<X_READ>` token right before the context so it knows this is reference material, and sees `<A_ASK>` before the question (if we choose to use it) – although the model might not inherently understand these tags unless we explain them in the prompt, they serve as fixed indicators consistent with the tokenizer’s training. We will experiment with including `<A_ASK>` for clarity (since it’s in the vocab, possibly from fine-tuning data), but if it confuses the model, we may omit it in the final prompt and just include the user question plainly or as a “User:” role.
    
- _Ensuring `<Z_RECEIVE>` in Output:_ We want the model’s answer to explicitly start with the `<Z_RECEIVE>` token (which will signal the beginning of Jacklyn’s answer in the system). To enforce this, our prompt or system instruction will say: _“Begin your response with `<Z_RECEIVE>`.”_ In addition, after generating the text, our backend can double-check and prepend `<Z_RECEIVE>` if the model somehow didn’t include it. (During testing, we’ll verify if the model follows the instruction – if not, a simple string concatenation can ensure the final output has the header token). The rest of the answer should then follow Jacklyn’s report format (e.g. `D.A.D.D.Y.S-H.A.R.D #X — Analysis Report\nSubject: ...\n...\n—JV`).
    

By constructing the prompt carefully and using the special tokens in context, we guide the LLM to produce a **persona-specific, formatted answer**.

**3. Retrieval Integration:** To supply the `<X_READ>` context, we need to hook into our retrieval system. There are two possible integration approaches:

- _Backend-Orchestrated Retrieval:_ Extend the **Python backend** (or whatever environment houses the embedding logic) to handle a query. For example, implement a method `retrieve_context(query)` that computes an embedding for the user’s question (using the same embedding model as was used for pages) and performs a similarity search against the stored page embeddings in Cassandra. For a minimal solution, a brute-force scan of all 710 page vectors can be done – the dataset is small – using cosine similarity. (This could be optimized later with an index or by filtering by symbol if the query explicitly mentions a character, etc.) The function would return the top few pages’ content or summaries. We can leverage the existing `RAGService` logic if present (the codebase hints at a `build_context` method that likely already does something similar). In fact, calling `get_rag_service().build_context(character_id="jacklyn-variance", user_query=...)` might produce a context object containing a summary of relevant info (which the Jacklyn agent was meant to use). If that is available, using it would accelerate development. Otherwise, implement a simplified version: query Cassandra for all embeddings (or use a CQL `WHERE` clause if a vector search extension is available), rank by similarity to the query embedding, and fetch the text of the top 3–5 pages.
    
- _Bun/Node Integration:_ As an alternative (especially if we want to avoid introducing a separate running Python service), we can perform retrieval in the **Bun server** itself. We could use a Node Cassandra client to fetch all page vectors (stored as floats) and do the similarity math in JavaScript. Given the small scale, this is feasible. However, writing that from scratch might be more work than reusing the Python code. A pragmatic approach is to expose a small **Python API** (e.g. via FastAPI running locally) for retrieval and questioning, and have the Bun server call that over HTTP. Since the repository already has Python code for Jacklyn’s agent, standing up a lightweight API around it could make sense. For now, we’ll plan on a simpler route: _call the existing Python functions directly._ Bun can spawn a child process to run a Python script (for example, a CLI that accepts a query and prints results). We could create a script `ask_jacklyn.py` that uses the `JacklynService` to get an answer (stream or final) for a given query. Then Bun’s endpoint can invoke this script and capture the output. This approach uses the tested Python logic without needing a persistent server, albeit with some overhead per call. It’s suitable for initial implementation and testing.
    

Regardless of approach, **unit tests** should verify that the retrieval component returns reasonable context for known queries (e.g., given a query about a specific character, the top result should be a page involving that character). Since the corpus is static, we can craft a test where we know which page should come up.

**4. API Endpoint & Frontend Wiring:** Introduce a new API endpoint to handle the user’s query and produce the response. We’ll name it something like `POST /api/ask` (or `/receive`). This endpoint will coordinate the entire pipeline:

1. **Receive Request:** The client (front-end or CLI) sends the user’s question (and perhaps some context like the current page the user is on, if available, which could help retrieval but is optional).
    
2. **Retrieve Context:** The handler calls the retrieval logic to get relevant corpus text. For instance, it might internally call `retrieve_context(query)` which returns a few paragraph-long snippets. These snippets are concatenated (with separators or bullet points) and will form the `<X_READ>` part of the prompt. We will also keep an eye on length – if the combined snippets are too long, truncate or summarize them. The tokenizer’s `count_tokens` function can assist here to ensure we don’t overflow the model’s context window.
    
3. **Formulate Prompt & Generate Answer:** Using the LLM wrapper from step 1 and the prompt strategy from step 2, the endpoint prepares the final prompt and requests a completion from the model. This may be done synchronously (the request waits for the model to finish) or asynchronously/streaming. Initially, a simple synchronous call that returns the full answer is fine. The model’s raw answer (string) is captured. If the answer doesn’t already contain `<Z_RECEIVE>` at the start (perhaps the model forgot it), prepend it manually for consistency.
    
4. **Send Response:** The endpoint responds with the generated text. The front-end will receive this and display it as Jacklyn’s message. The response format can be JSON (e.g., `{ "answer": "<Z_RECEIVE> ... analysis ... —JV" }`) or plain text. JSON is safer for future extensibility (we might include metadata later), so we’ll likely use JSON with an `"answer"` field.
    

On the **frontend side**, we will update the chat interface to use this endpoint. Instead of the `generateJacklynResponse` stub, the `handleSendMessage` function will do something like:

```ts
const res = await fetch('/api/ask', { method: 'POST', body: JSON.stringify({ query: inputValue }) });
const data = await res.json();
const jacklynReply = data.answer;
```

Then use `jacklynReply` to create the chat message in state. We should preserve the existing behavior of the UI (such as showing the “JV” avatar, etc.), which already works if we label the message with `type: 'jacklyn'`. We also likely want to keep the initial greeting behavior as is (that can remain a local initialization, or we could also generate it dynamically by calling the API with a special “hello” prompt – but that’s not necessary for now).

We will test the end-to-end flow by running the app and asking a few questions: Jacklyn’s responses should come back with appropriate content from the story (ensuring retrieval worked) and in the correct voice.

**5. Testing Strategy:** We will create tests at multiple levels to ensure the Step 4 implementation is robust:

- _Unit tests for LLM wrapper:_ Simulate both a local and fallback scenario. For example, monkey-patch the Ollama call to return a fixed string (e.g., `"Test response"`), and verify that our wrapper returns that string when Ollama is “available.” Then simulate Ollama throwing an error (or returning a failure status), and monkey-patch the OpenAI API call to return a different fixed string – verify the wrapper returns the fallback string and logs the fallback event. This ensures our conditional logic and error handling works as expected. We’ll also test that if the model returns text without the `<Z_RECEIVE>` prefix, the wrapper (or a post-processing step) correctly adds it.
    
- _Prompt format tests:_ Write a small function to build the prompt given some dummy context and query, and assert that the string contains the `<X_READ>` token followed by the context text, and `<A_ASK>` or the query in the right position. This is essentially testing our prompt assembly. We can include a known snippet in the context and ensure it appears unchanged in the prompt (no accidental modifications). If using a chat API (like OpenAI’s with system/user roles), test that our message array is constructed with the intended instructions.
    
- _Retrieval function tests:_ If we implement a pure function for retrieval (e.g., something that takes a query and returns a list of page IDs or texts), we can unit-test it by seeding it with a small fake dataset. Alternatively, in an integration test, run the `seed_embeddings.py` on a test keyspace (or use an in-memory list of vectors) and query it. For simplicity, we might skip an elaborate simulation since the real Cassandra integration is harder to test in CI – instead, we could abstract the similarity search into a function and test that given a set of sample embeddings it returns the closest ones. (For example, embed a few known sentences and see that a query returns the expected nearest neighbor.)
    
- _API endpoint tests:_ Using the development server (perhaps in a test environment with a smaller model or a stubbed LLM), make test POST requests to `/api/ask`. Provide a fake query like “Hello” or “Who is London Fox?” and verify the response JSON has an `"answer"` field starting with `<Z_RECEIVE>` and containing plausible content. We might stub out the retrieval and LLM for this test by temporarily patching them (so the test isn’t dependent on the actual model running). The idea is to ensure the endpoint returns a 200 and the output format is correct.
    
- _Frontend integration:_ Because the front-end now relies on the `/api/ask` endpoint, we should do a manual integration test in the browser to confirm everything works together. (Automated end-to-end tests could be added later if needed.) Open the app, trigger the Jacklyn chat, and submit a query; observe that the response appears and looks formatted. We should test a query that we know has a relevant page to ensure the agent indeed draws from the corpus (for example, ask something about “Glyph Marrow” and see if Jacklyn references relevant info from Glyph’s pages). This confirms that retrieval + LLM are actually connected and functional in the live setting.
    

**6. Logging & Monitoring:** As we implement, we will insert logging at key points of the Step 4 pipeline to aid in debugging and future maintenance:

- _Query Receipt:_ Log an info-level message when a new query is received by the `/ask` endpoint (include a truncated version of the query text, and perhaps a session/user identifier if available). This helps trace user interactions in the logs.
    
- _Retrieval Summary:_ After the retrieval step, log which pages (or character symbols) were selected as context. For example: _“Retrieved 3 pages for query ‘X’; symbols = [glyph-marrow, jacklyn-variance]”_. This will show that the system is pulling the expected context and can be useful to diagnose if irrelevant pages are being picked.
    
- _LLM Call:_ Log which backend is used and the prompt token counts. For instance: _“Calling LLM: backend=Ollama, prompt_tokens=512”_ or _“Using OpenAI GPT-4 for Jacklyn’s response (Ollama fallback)”_. If using streaming, we might log the start and end of generation and any partial outputs if needed for debugging.
    
- _Result:_ Log a short snippet of the generated result and its length. For example: _“Jacklyn response generated (length 150 tokens)”_. We should be careful not to log entire responses at debug level in production (to avoid clutter and leaking too much story content in logs), but a snippet or summary is fine for verification.
    
- _Errors:_ Any exceptions (failure to retrieve, model API errors, etc.) should be caught and logged with stack traces at error level. Our error handling will also ensure the API returns a safe error message to the client (perhaps “Jacklyn is unavailable at the moment, please try again” along with a `<Z_RECEIVE>` header to maintain format, if the error happens after some processing).
    
- _Moderation/Filters:_ (For completeness, though a minimal implementation might not include AI moderation, the codebase mentions moderation checks.) If we implement any content filtering on the query or response, log when a query is blocked or a response is sanitized. Initially, we likely skip full moderation, but it’s good to design with the possibility in mind.
    

These logs will be invaluable for later analysis (e.g., how often the fallback is used, how long responses take, whether the context retrieval seems to be working well, etc.). We will also ensure that logging from the Python side (if used) is not lost when called from Bun – if using a subprocess approach, we’ll capture stdout/stderr. In a more integrated solution (like a direct library call or an HTTP call), the logs would live in the respective service’s output.

**7. Design Considerations and Future Work:** The initial implementation focuses on **getting a correct, persona-aligned answer** for a single query. We purposefully keep it **single-turn**: the user asks, Jacklyn answers with no memory of previous Q&A (aside from any context included from the current page or session). The architecture, however, is being laid in a way that can evolve. For example, the Jacklyn service class in the code already hints at handling conversation history and even storing “memories” of the dialogue in Cassandra. Our Step 4 implementation will not yet use these advanced features, but we design the API such that we can extend it to multi-turn dialogues easily. We will ensure the endpoint can accept a conversation ID or history if needed later (for now, it can ignore it). We’ll also keep the **character ID (Jacklyn’s symbol)** as a parameter internally, so that in the future this service could potentially generate answers for other agents by switching the persona and context.

In terms of **Jacklyn’s narrative accuracy**, after deploying the basic solution, we will test and fine-tune the prompt. We may need to adjust the level of detail in the instructions. For example, if the model doesn’t automatically number reports or uses a wrong tone, we might add a more explicit format example in the system prompt (the code has a “strict format” enforcement routine we can draw from, which shows an example report template). The plan is to get a working baseline and then iterate on the prompt with real content.

Finally, we’ll add **documentation** for this new feature (how the `POST /ask` endpoint works, expected input/output) and ensure that any contributor can run the system (notes on needing an Ollama model or OpenAI key, etc.). With Steps 1–4 completed, we will have a fully functional QDPI loop: Jacklyn **reads** the corpus, **indexes** it via embeddings, the user **asks**, and Jacklyn **receives** (answers) with a response that is grounded in the story data and delivered in her distinctive voice.