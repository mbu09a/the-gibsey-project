# QDPI to DSPy Implementation Plan for the Gibsey Project

**Repo Audit:** The Gibsey Project is a single repository (monorepo) containing backend, frontend, docs, and scripts. Key components include:

- **Backend (FastAPI)** – `backend/` hosts the API service. In **`backend/main.py`**[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/main.py#L5-L13)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/main.py#L24-L32) a FastAPI app is configured, including routers for pages, prompts, users, search, **QDPI** endpoints, etc. The backend code (in `backend/app/`) defines data models (Pydantic) in **`models.py`**[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/models.py#L34-L43)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/models.py#L46-L54) such as `StoryPage` and `PromptOption`, and integrates a Cassandra database via Stargate. The API routes (e.g. **`app/api/pages.py`** for story page CRUD[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/api/pages.py#L11-L19)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/api/pages.py#L38-L46)) already allow basic page creation and retrieval. WebSocket support exists for real-time updates (`/ws/{session_id}` in **`main.py`**[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/main.py#L156-L164)). The **frontend** (React) resides in `src/` (e.g. `src/components/...`), and **gibsey-canon/** contains extensive design docs and the story corpus (the 710 canonical pages appear as Markdown under `gibsey-canon/corpus/characters/...`).
    
- **Cassandra & Stargate** – The project uses Apache Cassandra (with optional DataStax **Stargate** API). A keyspace (default name `gibsey_network` in code, or `gibsey` per env) is created by **`backend/app/cassandra_schema.py`**[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_schema.py#L51-L60) or its optimized v2 counterpart. The schema in **`cassandra_schema_v2.py`** defines tables like `story_pages`, `pages_by_symbol`, `pages_by_section`, etc.[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_schema_v2.py#L73-L81)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_schema_v2.py#L99-L108). In production, a **CassandraDatabase** class (see **`cassandra_database.py`**[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_database.py#L34-L42)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_database.py#L58-L66)) and an async **StargateClient** (**`stargate_client.py`**[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/stargate_client.py#L24-L32)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/stargate_client.py#L76-L84)) are used to read/write data via REST. Environment variables `CASSANDRA_HOSTS`/`STARGATE_URL` and `CASSANDRA_KEYSPACE` control the connection[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_database.py#L34-L42). Currently, the development `.env` defaults to a mock DB (in-memory)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/.env.example#L7-L15), meaning Cassandra/Stargate must be explicitly enabled for the real pipeline.
    
- **Data model & Existing Schema** – Story pages are stored in the `story_pages` table (see **`cassandra_schema.py`**[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_schema.py#L77-L86)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_schema.py#L93-L101)) with fields for `id` (page ID like “S01-P0007”), `symbol_id` (character identifier, e.g. `"london-fox"`), `rotation` (orientation in degrees, e.g. 0, 90, 180, 270), `page_type` (`primary` story text vs. `user_query`, `ai_response`, etc.), `parent_id` (for branching), `branch_id`, timestamps, and content text. Secondary index tables (`pages_by_symbol`, `pages_by_parent`, etc.) support querying. The **canonical 710 pages** (“The Entrance Way”) are intended to be ingested as the initial corpus – they exist as Markdown/JSON in the repo (e.g. `gibsey-canon/corpus` and `src/assets/texts.json`). Currently, the schema doesn’t explicitly separate “edges” or track return loops; instead, `parent_id`/`child_ids` in `StoryPage` are used to form narrative links. One goal is to introduce explicit edge tracking (NEXT, LOOP, FORK relations) and branch management (e.g. return debt counters per section).
    
- **Embedding & Indexing** – There is partial vector embedding support. The `StoryPage.embedding` field is a list of floats (e.g. 384-d SBERT vector)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_schema.py#L94-L101). A **VectorEmbeddingService** in **`vector_service.py`** can generate embeddings using SentenceTransformers or OpenAI[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/vector_service.py#L31-L39)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/vector_service.py#L98-L106). In optimized schema v2, a separate `vector_embeddings` table is defined for semantic search[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_schema_v2.py#L137-L146). Additionally, the repo contains a lightweight local **EmbeddingStore** (`src/ai/memory/embedding_store.py`[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/src/ai/memory/embedding_store.py#L18-L27)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/src/ai/memory/embedding_store.py#L31-L39)) that writes embeddings to a JSONL for in-memory vector search (used for “Jacklyn’s memory”). For MVP, the plan is to store page embeddings in Cassandra (possibly as BLOB or float list) and implement basic semantic index signals (entities, motifs, novelty, coherence) as stubbed placeholders.
    
- **LLM Integration** – **`backend/app/llm_wrapper.py`** provides a unified interface to language models[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/llm_wrapper.py#L21-L29)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/llm_wrapper.py#L44-L52). By default it uses **Ollama** (local LLaMA v3 8B model) as primary backend[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/llm_wrapper.py#L22-L30), falling back to OpenAI (GPT-4) or Anthropic (Claude) if configured[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/llm_wrapper.py#L56-L64)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/llm_wrapper.py#L74-L83). This wrapper handles prompt composition (system + user prompts) and returns an `LLMResponse` object with text and token counts[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/llm_wrapper.py#L126-L135)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/llm_wrapper.py#L141-L150). The repository’s `.env.example` shows config for OLLAMA and API keys[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/.env.example#L20-L28)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/.env.example#L44-L52). This LLM wrapper will be invoked in the **Receive (Z)** stage of the QDPI pipeline to generate new page text under the right voice and constraints. (Currently, an older `ask` endpoint and RAG logic exist in `app/api/ask.py` and `app/retrieval.py`, but these will be superseded by the new DSPy-driven pipeline.)
    
- **API & Services** – The FastAPI backend is modular. In addition to `pages` and `prompts` endpoints, there are **QDPI** endpoints: **`app/api/qdpi.py`** implements a **symbol encoder/decoder API**[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/api/qdpi.py#L70-L78)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/api/qdpi.py#L118-L127) using a `qdpi_engine` (with 64 base symbols × 4 rotations × 4 parity = 1024 glyphs, though the interactive API uses 256 glyph IDs without parity by default). There’s also `qdpi_ux.py` and ECC-related endpoints (for extended error-correcting code functionalities). These indicate an existing framework for the 256-symbol scheme as a general encoding system. However, what’s needed now is to integrate that scheme into the story generation _pipeline_ (X→Y→A→Z cycle). We will likely add new API routes (under, say, `/api/v1/pipeline` or similar) for running the QDPI pipeline steps (run symbol, branch, etc.) as described in the plan.
    
- **Kafka & Streaming** – There is **no active Kafka integration yet in the code** (no Kafka client usage in the repository code). Event streaming is mentioned in docs (see `cassandra kafka integrations.md` in gibsey-canon) and a `cluster_events` table exists for logging events[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_schema.py#L212-L220), but background workers or message queues are not implemented in this MVP. We will ignore Kafka for now (point 7 optional) and focus on synchronous pipeline execution within the FastAPI service, possibly adding Kafka hooks later for audit trails or asynchronous tasks.
    
- **Configuration & DevOps** – Config is managed via environment variables (`.env` with `DATABASE_URL`, API keys, etc.) loaded at startup[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/main.py#L18-L26). There are scripts to initialize DB (**`scripts/cassandra_init.cql`** and **`backend/scripts/init_db.py`**[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/scripts/init_db.py#L80-L88)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/scripts/init_db.py#L92-L100)) which create keyspace and tables, plus a **`scripts/seed_embeddings.py`** to ingest the corpus into the vector store[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/scripts/seed_embeddings.py#L44-L52)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/scripts/seed_embeddings.py#L64-L72). Continuous integration is configured with GitHub Actions (`.github/workflows/`), and some tests exist (e.g. **`backend/test_jacklyn_pipeline.py`** for complex scenario validation of Jacklyn’s responses). We will add new unit tests for the QDPI pipeline components (encoding, policy, trajectories) and ensure existing tests continue to pass.
    

---

## Implementation Plan (QDPI→DSPy Pipeline)

### 0) Assumptions & Inputs Needed

-  **Repo access** – Confirm the correct repository URL/branch to use (assuming `mbu09a/the-gibsey-project` main branch) and a local clone path for running scripts.
    
-  **Cassandra environment** – Confirm Cassandra is running (contact points, port). If using Docker, ensure `CASSANDRA_HOSTS` (or `DATABASE_URL`) is set (e.g. `cassandra:9042` in Docker, or `localhost:9042` if running locally) and a keyspace name. We assume keyspace **`gibsey`** (per .env) or **`gibsey_network`** – please clarify which to use so schema and code align.
    
-  **Embedding model/dimensions** – Decide on the embedding model for MVP. The repo defaults to SBERT `all-MiniLM-L6-v2` (384 dims)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/vector_service.py#L52-L59). We will proceed with 384-dim vectors (or 768 if using mpnet) and store them as a blob or float list in Cassandra. (Assumption: fine to start with local SBERT; OpenAI embeddings optional if API key provided.)
    
-  **Canonical corpus source** – Provide the canonical 710 pages content and a section mapping. We need either the `texts.json` (if it contains all pages) or confirm the path to the Markdown files (`gibsey-canon/corpus/...`). This is required to ingest the initial pages into the database (Section 1 through 16 with page numbering). We also assume a mapping of page IDs like `S01-P0001` etc., and knowledge of how pages are segmented into 16 sections. If this is defined in an existing metadata file (e.g. `character_architecture_mapping.json` in corpus metadata), please supply it or confirm how sections are identified for each page.
    

_Assumptions beyond MVP:_ Cassandra 4.x is used (no native VECTOR type), so we’ll treat embeddings as a blob or list for now. Stargate is available at `localhost:8082` (per .env defaults) – if not, we’ll use the Python driver directly. Also, we assume the QDPI symbol codex (16 characters × 16 behaviors) is as given in the prompt, matching the names in code (`an_author`, `london_fox`, etc.). If any character naming differs (underscores vs. spaces), we’ll normalize in the mapping.

### 1) Data Model Bring-up

-  **Keyspace and Tables:** Create the keyspace (if not already) and apply the minimal schema. Using CQL (see script below), define a **`pages`** table for core page info, a **`page_edges`** table for relationships, an **`page_index_signals`** table for indexing metadata, etc. (We will use the schema outlined in the next section, adjusting for our keyspace name and Cassandra version.) Run this via cqlsh or a migration script.
    
-  **Migration script:** Write a Python or CQL migration (e.g. `scripts/migrate_qdpi_schema.py`) that connects to Cassandra (using `cassandra.cluster.Cluster` as in `CassandraSchemaManager`) and executes the schema DDL. This script should log table creations and verify the schema (similar to `verify_schema()` in `cassandra_schema.py`[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/cassandra_schema.py#L270-L278)).
    
-  **Ingest canonical ring:** Populate the `pages` table with the 710 canonical pages. We’ll write `scripts/ingest_canon.py` to load the source text for each page (from JSON or Markdown files) and insert records. Each page gets a stable `page_id` (e.g. "S01-P0001"), `symbol_code` (e.g. 0x00 for the first page of an author, etc.), and `section` (1–16). Set `provenance='C'` and `orientation='X'` for these canon pages. Also insert the linear NEXT edges: for page `S01-P0001` to `S01-P0002`, etc., and for the last page of Section 16 linking back to S01-P0001 to complete the ring. (Mark all canon pages with `trajectory='T0'` by default, meaning they continue within canon.) Verify that all pages and next edges are inserted correctly.
    

### 2) Core Library Implementation

-  **QDPI symbol utils (`qdpi/symbols.py`):** Implement encode/decode functions for the 256-symbol scheme. For example, `encode_symbol(char_hex: int, behavior: int) -> int` that returns `(char_hex << 4) | behavior`, and `decode_symbol(code: int) -> (char_hex, behavior)`. Also provide a dictionary or list for the 16 behavior codes (0–15) mapping to `{orientation, provenance}` pairs[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/api/qdpi.py#L72-L81)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/qdpi.py#L24-L33) – e.g. 0:`XC` (“Read, Canon”), 1:`XP`, ... 15:`ZS`. This will allow translating a `symbol_code` into human-readable type info. We’ll also include mapping of `char_hex` 0–15 to character names (from “an author” to “The Author”) as given in Context A.
    
-  **Page schema model (`qdpi/page_schema.py`):** Define a Pydantic model (or simple dataclass) representing the _page YAML schema_ given (Context G). This includes fields like `page_id`, `symbol_code`, `character` (name), `orientation`, `provenance`, `trajectory`, `voice_preset`, `inputs` (seeds, motifs, constraints), `text`, `edges` (with keys `next_within`, `loop_back_to`, `forks`), and `validation` flags. While the API will primarily use JSON, having this schema helps with validation and with formatting outputs (e.g. converting stored DB page + edges + signals into this structured form if needed).
    
-  **Policy logic (`qdpi/policy.py`):** Implement the runtime policy invariants (Context D) in a small utility module. For example, a function `enforce_return_policy(page: Page) -> bool` that checks if a new branch can be created or if a return (T1 loop-back) is mandated (e.g. if `return_debt` counter is high). Also functions to manage **section_counters**: increment open branch count when forking (T2/T3) and increment return_debt when a branch is started, decrement when a branch returns. We will keep this simple for now – e.g., define thresholds for `MAX_RETURN_DEBT` (maybe 2) and `MAX_BRANCH_LENGTH` (TTL, say 3 pages) and provide checks like `policy.allow_fork(section_id)`. These stubs will be used by the `select_traj` gating in the DSPy module to decide trajectories.
    
-  **Indexing module (`qdpi/index.py`):** Create a placeholder for indexing logic. This should include functions like `extract_entities(text)`, `extract_motifs(text)`, `find_unresolved_anchors(text)` – currently they can return empty or dummy values (since NLP is out of scope for MVP). Also include a function to compute `novelty` and `coherence` scores (for now, perhaps random or fixed values), and to generate an embedding vector. In MVP, we can call the `VectorEmbeddingService` to embed the text (if performance is an issue, we can skip real embedding and use a fixed vector). The output of `index.py` will feed into the **IndexSignature** (Y) – i.e., produce an `entities` set, `motifs` set, `unresolved_anchors` (if any), plus `novelty`/`coherence`. These signals help the trajectory selection logic. All of this can be stubbed initially (e.g. return empty sets and default scores), with TODOs to implement later. We will still insert whatever signals we have into the `page_index_signals` table for completeness.
    

### 3) DSPy Program Integration

-  **Define DSPy Signatures:** Following Context H, add the four signature classes `ReadSignature, IndexSignature, AskSignature, ReceiveSignature` (likely in a new module, e.g. `qdpi/pipeline_signatures.py`). These should subclass `dspy.Signature` and declare the I/O fields for each stage (as given in the prompt)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/api/qdpi.py#L70-L78)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/api/qdpi.py#L118-L127). For instance, `ReadSignature` might take `symbol_code: int` and output `render: str, summary: str, edges: dict, context: dict`. `AskSignature` takes the current state and optional user intent and outputs a plan (trajectory, voice, constraints, sources). We’ll keep these classes minimal – mostly type definitions for DSPy to use.
    
-  **QDPIProgram module:** Implement the `QDPIProgram` class inheriting `dspy.Module` (e.g. in `qdpi/program.py`). In its `__init__`, instantiate the pipeline components: `self.read = dspy.Predict(ReadSignature)`, `self.index = dspy.Predict(IndexSignature)`, `self.ask = dspy.Predict(AskSignature)`, `self.receive = dspy.ChainOfThought(ReceiveSignature)`, and a `self.select_traj = dspy.Select(options=["T0","T1","T2","T3"])` for trajectory gating[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/qdpi.py#L78-L86). Then implement `forward(self, symbol_code: int, user_intent: str = "")` to orchestrate one full cycle:
    
    1. Call `x = self.read(symbol_code)` – this will render the page (`render` could be the page text if symbol is a read type, or empty if not applicable) and provide a summary and any immediate edges (for canon pages, `edges.next_within` is known). For generative symbols, `Read` might do nothing. We can stub `read` such that if the symbol’s orientation is Read (X), it fetches the page text from DB; otherwise it just returns an empty context.
        
    2. Call `y = self.index(x.render)` to index the content (or if x.render is empty, possibly use the last page text). This produces embedding, entities, etc., or stubs.
        
    3. Call `plan = self.ask(state={"x": x, "y": y}, user_intent=user_intent)`. For MVP, `ask` can produce a trivial plan: e.g. copy over `user_intent` and propose a trajectory (maybe always “T0” unless user_intent says otherwise) – we will refine later. The `voice` and `constraints` fields can be filled with defaults (e.g. a voice preset based on the character, and max_tokens from config).
        
    4. Determine trajectory: `plan.trajectory = self.select_traj(context={"scores": y.scores, "intent": user_intent})`. Here we use the policy from step 2 – for now, we can simply choose “T0” (continue canon) unless there are unresolved anchors requiring a loop (T1) or an explicit user fork (T2/T3). This is a placeholder; in the future, this selection could examine `novelty` vs `coherence` to decide if we should branch out.
        
    5. Call `z = self.receive(plan)` – this triggers text generation for the new page. We’ll implement `receive` such that it uses the LLM wrapper to produce `page_text` and then populates `edges` (like setting `next_within` or `loop_back_to` in the output) and a `validation` dict. For MVP, we might simply echo a dummy text or a single line like `"[Generated content for plan]"` to verify the pipeline works, and mark `validation.style_ok=True`. The real integration would involve calling `LLMWrapper.generate_response(plan_prompt)` where `plan_prompt` is assembled from the plan (voice, constraints, etc.).
        
    6. After receiving, we call `_ = self.index(z.page_text)` to index the newly generated page and store its signals. (Even if we don’t use the result `_`, this ensures the new content is embedded and indexed immediately.) Then return `z` (which contains the `page_text`, edges, etc.).
        
-  **Stub implementations for pipeline steps:** Initially, implement minimal logic in `Read, Ask, Receive`:
    
    - `read(symbol_code)`: If the symbol’s orientation is `X` (read), query the `pages` table for the latest page with that symbol (or if it’s a canon page, just load its text). Otherwise, return an empty render or a generic message. (For now, we might not need elaborate summarization – perhaps just set `render = page.text` for canon, and an empty string for symbols that don’t involve reading existing text.)
        
    - `ask(state, user_intent)`: This could simply combine the `user_intent` with a default next-step plan. For example, if `user_intent` is provided and symbol’s orientation is `A` (Ask), the plan might be to incorporate that intent. Otherwise, if we are continuing canon (T0) or looping back (T1), the plan might just carry the necessary reference (like `return_to` for a T1). In MVP, have `plan = {"trajectory": "", "voice": {"character": "...", "style": "narrative"}, "constraints": {"max_tokens": 200}, "sources": []}`; we’ll fill `trajectory` after select_traj.
        
    - `receive(plan)`: Use `LLMWrapper` here. Construct a prompt from the plan (e.g. for trajectory T0 or T1, maybe just ask the model to continue the story; for T2/T3, the prompt might indicate a more divergent path). For MVP, we can call `await llm.generate_response(prompt)` and take the `.text`. Then create a new page_id for the output (e.g. if branching, a new P##### in the appropriate section; if continuing, the next canon ID). Fill `edges`: e.g. if T0, set `edges.next_within = <new_page_id>` linking from current to new; if T1, set `loop_back_to = return_to` (from plan); if T2, set `forks = [new_page_id]` plus ensure a loop-back edge from the generated branch end to `return_to`. Validation can include checking the style (we might simply always pass `style_ok: True` and maybe include a `continuity_score` as 1.0 for now, with a TODO to compute it). Finally, persist the new page and edges to the database (likely via the same Cassandra client in `database.py`, e.g. calling `db.create_page(StoryPage(...))` and adding edges entries). This persistence can be done here or in the API layer after `QDPIProgram.forward` returns. For simplicity, we might do it in the FastAPI endpoint after getting `z`.
        
-  **Trajectory gating (select_traj)**: For now, implement a simple rule: always choose **T0 Continue** unless a special condition. We could, for example, if `user_intent` is non-empty and asks for something off-story, select T2 (fork), or if the current page has unresolved hooks, select T1 for a loop/bridge. In code, something like:
    
    `if y.unresolved_anchors and not plan.trajectory: chosen = "T1" elif user_intent and "fork" in user_intent.lower(): chosen = "T2" else: chosen = "T0" plan.trajectory = chosen`
    
    The `dspy.Select` can be given a custom function or we can set it post-hoc as above. This is enough to demonstrate the mechanism; refining the gating logic is a future task.
    

### 4) API Surface (FastAPI Endpoints)

-  **POST `/run/symbol/{code}`:** Implement an endpoint to trigger a full QDPI pipeline run for a given symbol code (0x00 – 0xFF). For example, **`app/api/pipeline.py`** with:
    
    `@router.post("/run/symbol/{code}") async def run_symbol(code: str, user_intent: str = ""):     # decode code from hex if needed     symbol_int = int(code, 16) if isinstance(code, str) else int(code)     result = await qdpi_program(symbol_int, user_intent=user_intent)     # Save result (page and edges) to DB     ...     return {"page_id": result.page_id, "text": result.page_text, "edges": result.edges, "trajectory": result.plan.trajectory}`
    
    Here `qdpi_program` is an instance of `QDPIProgram` (probably created at app startup and injected, or we call QDPIProgram().forward each time). The endpoint should parse the symbol code (allowing both hex string like "0x1A" or decimal) and optional JSON body with `user_intent`. After running the pipeline, it returns the new page’s data (at least the ID so the client can fetch details). We must also handle if the symbol code corresponds to a **Read (X)** operation on an existing page type – in that case, the pipeline might not generate a new page but just perform reading or summarization. To keep MVP simple, we could restrict `/run/symbol` to symbols that produce output (e.g. Ask or Receive symbols), but ideally it should handle all. Perhaps for X (read) symbols, it just returns the page content and edges without generating new content.
    
-  **GET `/page/{page_id}`:** Provide an endpoint to fetch a page’s details, including its text, type, and edges. We already have `GET /pages/{id}` returning a `StoryPage`[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/backend/app/api/pages.py#L38-L46). We can either reuse that (maybe augment it to also gather edges and index signals from the new tables) or implement a new route that joins data from `pages`, `page_edges`, and `page_index_signals`. Likely easiest is to extend the existing `get_page` to assemble a full page view: after getting the StoryPage, do a query on `page_edges` (with page_id as partition key) to collect all edges, and on `page_index_signals` to get any indexing info. Then return a combined JSON (perhaps conforming to the YAML schema format). For now, simply returning the StoryPage (which includes child_ids, etc.) might suffice for basic needs, but to adhere to the new structure we’ll incorporate the additional info.
    
-  **POST `/branch/{page_id}`:** This endpoint initiates a branch from a given page. Essentially, it runs the pipeline’s Ask→Receive steps starting at `page_id`. For example, if a user is at page X and wants to explore a “what if” scenario, the frontend would call POST /branch/X (with maybe some user prompt). The handler for `/branch/{page_id}` will: lookup the page by id (to know which character/section it’s in), determine the appropriate symbol to use for branching (likely “Ask, User (AU)” or “Receive, User (ZU)” symbols associated with that character), and then call `qdpi_program.forward(symbol_code, user_intent)` for that symbol. Alternatively, we could allow the client to specify which trajectory type they want (T2 or T3). For MVP, we might assume any branch is a T2 Fork to a new section (unless the branch is meant to loop back which is T1 and would use /run directly with a ZU symbol maybe). Implementation: find the character of `page_id` (from DB), choose behavior = **Ask:User (AU)** which is nibble 0xA for that character’s symbol, then run pipeline. The response would be similar to /run: return the new page created as the branch start (and likely a plan indicating where it will return). We also update `section_counters.open_branches++` and `section_counters.return_debt++` for the section that was forked from.
    
-  **GET `/stats/section/{id}`:** Implement an endpoint to retrieve branching stats for a given section (or for all, if id = “all”). This will query the `section_counters` table for the given section number and return counters for `open_branches` and `return_debt`. If using Cassandra counters, this is straightforward. If not using actual counter types, we maintain these counts manually on each branch/return operation (in the code or in a small cache table). For MVP, it’s acceptable to simulate these (even just always return 0 for now, to be filled in as branching is utilized). The endpoint will help monitor the narrative health (ensuring we don’t leave too many threads hanging).
    

### 5) Tests & Fixtures

-  **Unit tests – encode/decode:** Add tests to verify that all 256 symbol codes round-trip correctly. Iterate 0–255, ensure that `decode_symbol(code)` returns `(char, beh)` and then `encode_symbol(char, beh)` returns the original code. Also test boundary values (0x00 and 0xFF specifically for correctness). These tests ensure no off-by-one in bit logic.
    
-  **Unit tests – policy invariants:** Write tests for `qdpi/policy.py`. For example, if `return_debt` exceeds threshold, our policy should recommend no new forks (simulate by setting a high debt and see that `policy.allow_fork()` returns False). Also test TTL enforcement: simulate a branch with 3 generated pages and verify the next trajectory selection forces a return (T1). Since our policy is simple, we might create dummy counters and call the policy functions to verify outcomes.
    
-  **Integration test – golden path:** Construct a test that runs the pipeline from a known starting page and validates a full cycle. For instance, given the very first page (S01-P0001, symbol 0x00), run `/run/symbol/0x00` and expect it to produce page S01-P0002 (the next canon page) with trajectory T0. Then run `/run/symbol/0x00` again without user intent and ensure it proceeds sequentially. Next, test a loop: take a page that has an unresolved anchor (if our indexing can simulate one) and ensure the pipeline chooses T1. If needed, we can force a condition: e.g., manually mark an anchor and see that the next call yields a loop-back plan. Also test the branch endpoint: call `/branch/{page_id}` on some page and verify that a new page is created in a different section (T2) and that `section_counters` updated. These integration tests can use the FastAPI TestClient or directly call `QDPIProgram.forward` with dummy data, checking that DB entries are created.
    
-  **Fixtures and Mocks:** For tests, we may want to stub out the LLM generation to make results deterministic. For example, monkeypatch `LLMWrapper.generate_response` to return a fixed string (e.g. `"Test output"`) so that tests aren’t flaky. Also, use a test keyspace or an in-memory mock DB for faster tests (the code’s `MockDatabase` could be leveraged by setting `DATABASE_URL=mock://...`). Ensure to reset any counters or tables between tests (we can use Cassandra TRUNCATE for the keyspace or just use the mock DB mode).
    

### 6) CLI & Developer Scripts

-  **Ingestion script (`scripts/ingest_canon.py`):** As noted in step 1, implement a script to bulk load the canonical story. Likely reuse code from `seed_embeddings.py` and `ingest_entrance_way.py` but adapt to use our new `pages` and `page_edges` tables. This script should assign `section` numbers and `page_num` within section for each page, set the appropriate `symbol_code` (the left 4 bits derived from character mapping, and right 4 bits = 0 for XC since these are canon reads). It should also mark each as canonical (provenance C) and orientation X. After inserting all pages, it should link them via edges (for each page except the last in section, add a NEXT edge to the next page; for the last in section, NEXT edge to the first page of next section; and for the very last page of Section 16, NEXT back to first page of Section 1). We should also output a summary (count of pages ingested, etc.).
    
-  **Run pipeline script (`scripts/run_symbol.py`):** Provide a convenient CLI to invoke the pipeline without the web API, for development. For example, `python scripts/run_symbol.py 0x8E --intent "Some user prompt"` would decode 0x8E, run `QDPIProgram.forward(0x8E, user_intent=...)`, and print the result. This script can help quickly test the pipeline logic from the console. It should initialize the necessary services (e.g. load environment, maybe start Cassandra session if needed, instantiate QDPIProgram, etc.). Keep it simple: parse args for symbol and intent, call the forward method, then print the new page text and any edges decided.
    
-  **Re-index utility (`scripts/reindex_page.py`):** (Optional) A small script to recompute and update the index signals for a given page or all pages. Since our MVP indexing is rudimentary, this might not be needed immediately. But it’s useful to have a tool that goes through all pages in the DB, runs `qdpi/index.py` logic on their text, and updates the `page_index_signals` table accordingly. This ensures consistency if we improve the indexing later. Mark this as a future enhancement; for this week, perhaps just scaffold the script.
    

### 7) (Optional) Kafka & Async Extensions

-  **Plan event topics:** Design Kafka topics for pipeline events, to integrate later. E.g. `qdpi.page.generated` (when a new page is created by Z), `qdpi.index.completed` (when indexing is done), `qdpi.trajectory.decision` (for each trajectory choice). Document these topics and the message schema (which could be as simple as JSON with page_id, timestamp, and relevant payload). We won’t implement actual Kafka publishing now, but by planning the topics, we ensure our pipeline code can hook in later.
    
-  **Consumer stubs:** Outline how a background consumer might work (for example, a Faust or FastAPI background task that listens for `qdpi.page.generated` events to perform additional processing like sending notifications or updating a search index). Again, this is a note for future—no active code needed in MVP except maybe logging events to console in the pipeline when they occur (which we can easily do within the pipeline steps for now).
    
-  **Audit log** (future): Optionally, consider using the existing `cluster_events` table to log each pipeline run (X, Y, A, Z as separate events). For now, we skip this, but we keep it in mind as a risk mitigation (if something goes wrong in generation, an event log could help debugging).
    

### 8) Definition of Done

- **End-to-end generation:** We can initiate a pipeline run via the API and get a coherent result. Specifically, calling `POST /run/symbol/0x00` (representing an author’s first canon page) returns a JSON with a new `page_id` and text. The new page is persisted in Cassandra (we can GET it subsequently), and appropriate edges are recorded (e.g. the `next_within` link from the previous page now points to this new page). We can also trigger branching: e.g. `POST /branch/S03-P0012` creates a fork from that page, and the new branch page is stored and linked back via a loop edge or `return_to` pointer.
    
- **Data integrity:** All new tables (pages, page_edges, etc.) exist and contain expected data. Section counters update when branches are made or closed. Index signals are stored (even if dummy values). Embeddings are generated for new pages and stored in `page_embeddings` (or the embedding field of `pages`), enabling future semantic searches.
    
- **Testing:** All new unit tests pass – verifying symbol encoding, policy gating, and a basic pipeline flow. No regressions are introduced to existing functionality (the old endpoints /pages, /prompts still work as before).
    
- **Minimal docs:** The plan and any new modules are documented in code comments or an updated `README.md` so that other developers can understand how to run the pipeline. For instance, update **backend/README.md** to include instructions for the new endpoints and how to run the ingestion script.
    
- **Performance check:** Running a single full cycle (X→Y→A→Z) should complete within a reasonable time (say < 5 seconds using local LLM or a small model) – this is informal, but we want to ensure the added layers (DSPy, etc.) don’t introduce blocking issues. If the pipeline is slow, we might mark it as needing optimization but still consider MVP done if it works correctly.
    

### 9) Risks & Mitigations

- **Vector storage limitations:** Cassandra 4.x doesn’t natively support vector similarity queries. We mitigate this by storing embeddings as blobs or lists and deferring ANN indexing – for now, we won’t attempt complex vector search in real-time. Later, when Cassandra 5 or a separate vector DB (like Chroma) is available, we will migrate the `page_embeddings` data and enable semantic search endpoints. In the interim, our `page_index_signals.novelty/coherence` scores can act as a proxy to decide when to branch or continue.
    
- **LLM output variance:** The quality and style of generated pages might vary unpredictably, risking narrative drift. To mitigate this, we include **constraints in the Ask step** (e.g. `max_tokens`, desired tone via `voice_preset`) and possibly use the moderation/voice-check features already in the code. We can implement a simple retry mechanism for generation: if the `validation.continuity_score` from `receive` is low or style checks fail, we could regenerate or adjust the prompt. For MVP, we’ll log any anomalies and set conservative limits (short generations) to keep output on track.
    
- **Policy bugs (branch explosion or dead-ends):** The rules for returning to canon and limiting branches are tricky – a bug could either stifle creativity (too strict) or lead to many open threads (too lenient). We mitigate this by starting with very simple, strict rules (e.g. at most one open branch per section at a time, TTL=3) and writing tests for those conditions. We’ll also include **monitoring via `/stats/section`** so we can observe if any section’s `open_branches` count is unexpectedly high. In case of issues, we can manually trigger returns or even temporarily disable T2/T3 trajectories until the logic is improved.
    
- **Integration complexity:** Combining DSPy, FastAPI, and the existing code could introduce integration challenges (threading, async, etc.). We mitigate by keeping the DSPy usage minimal (maybe even running the pipeline synchronously for now). We’ll instantiate the QDPIProgram at startup and reuse it, avoiding heavy re-initialization overhead. Also, if needed, we can bypass DSPy’s internal async by wrapping calls appropriately (or using `anyio` if DSPy expects sync). We should be ready to adjust if the DSPy library has quirks – since this is an MVP, in worst case we could manually chain the four steps without DSPy as a backup plan.
    
- **Data consistency:** Since we plan to write to multiple tables (pages, edges, signals, counters) on each generation, there’s a risk of partial updates if something fails mid-way. To mitigate, each pipeline run could use a simple transaction strategy: e.g. generate all data, then commit to DB in a single function (ensuring all or nothing). Cassandra doesn’t support multi-table transactions, but we can order the writes such that the main page goes last (so if edges or signals fail to write, we don’t have dangling page records). Additionally, log events so we can reconcile later if needed (for example, if a page was created without edges, a maintenance script could detect and fix it).
    

---

## **Proposed CQL Schema (Minimal QDPI Tables)**

`-- Keyspace (if not already created) CREATE KEYSPACE IF NOT EXISTS gibsey    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};  USE gibsey;  -- Core pages table: each page instance (canonical or generated) CREATE TABLE IF NOT EXISTS pages (   page_id        TEXT PRIMARY KEY,   -- e.g. "S01-P0007" or a UUID for branches   char_hex       TINYINT,           -- 0..15 (which character)   behavior       TINYINT,           -- 0..15 (which behavior code, 0=XC,...15=ZS)   symbol_code    TINYINT,           -- 0..255 = (char_hex<<4)|behavior (redundant, but handy)   section        INT,               -- Section number 1..16 (or 0 for out-of-canon miscellany)   page_num       INT,               -- Page number within section (null if ad-hoc branch with no fixed position)   title          TEXT,              -- Optional title or chapter heading   orientation    TEXT,              -- 'X'/'Y'/'A'/'Z'   provenance     TEXT,              -- 'C'/'P'/'U'/'S'   trajectory     TEXT,              -- 'T0'|'T1'|'T2'|'T3' (how this page was reached)   return_to      TEXT,              -- If this is a branch page, where it loops back (page_id)   text           TEXT,              -- The content of the page   created_at     TIMESTAMP,   author         TEXT,              -- 'AI' or 'user' etc., who created it   version        TEXT               -- e.g. '1.0' or schema/version info (if needed) );  -- Edges table: relationships from a page to other pages CREATE TABLE IF NOT EXISTS page_edges (   page_id   TEXT,        -- source page   edge_type TEXT,        -- 'NEXT' | 'LOOP' | 'FORK' | 'MIRROR'   target_id TEXT,        -- target page   created_at TIMEUUID,   -- time of creation (for ordering multiple edges of same type)   PRIMARY KEY ((page_id), edge_type, created_at) ) WITH CLUSTERING ORDER BY (edge_type ASC, created_at ASC);  -- Embeddings table: semantic vector for each page (if not kept in pages table) CREATE TABLE IF NOT EXISTS page_embeddings (   page_id TEXT PRIMARY KEY,   dim     INT,   embedding BLOB    -- stored as binary; e.g. np.array(float32) bytes );  -- Index signals table: store entities, motifs, anchors, and scores per page CREATE TABLE IF NOT EXISTS page_index_signals (   page_id            TEXT PRIMARY KEY,   entities           SET<TEXT>,   motifs             SET<TEXT>,   unresolved_anchors SET<TEXT>,   novelty            DOUBLE,   coherence          DOUBLE );  -- Section counters table: track branching stats per section CREATE TABLE IF NOT EXISTS section_counters (   section      INT PRIMARY KEY,   open_branches COUNTER,   return_debt   COUNTER );  -- Plans table: (optional) store plans for each Ask/Receive transaction (for debugging/audit) CREATE TABLE IF NOT EXISTS plans (   plan_id    TIMEUUID PRIMARY KEY,   page_id    TEXT,        -- the page that was generated or read   symbol_code TINYINT,    -- which symbol was executed   trajectory TEXT,        -- trajectory decided   plan_json  TEXT,        -- JSON blob of the plan details (sources, constraints, etc.)   created_at TIMESTAMP );`

> **Note:** If using Cassandra 5.0 with vector support, we could modify `page_embeddings.embedding` to a `VECTOR<FLOAT, 384>` and create an SAI index for ANN search[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/scripts/cassandra_init.cql#L14-L23)[GitHub](https://github.com/mbu09a/the-gibsey-project/blob/9d584661a571430f941fa26d7bed87430373d576/scripts/cassandra_init.cql#L28-L33). For now, we use BLOB for portability. The `plans` table is mainly for debugging and can be omitted in production if not needed.

## **Python Stubs and Skeletons**

`# qdpi/symbols.py – Encoding and behavior mapping CHARACTER_MAP = {     0: "an author",     1: "london-fox",     2: "glyph-marrow",     3: "phillip-bafflemint",     4: "jacklyn-variance",     5: "oren-progresso",     6: "old-natalie-weissman",     7: "princhetta",     8: "cop-e-right",     9: "new-natalie-weissman",     10: "arieol-owlist",     11: "jack-parlance",     12: "manny-valentinas",     13: "shamrock-stillman",     14: "todd-fishbone",     15: "the-author" } BEHAVIOR_MAP = {     0: {"orientation": "X", "provenance": "C"},   # XC: Read, Canon     1: {"orientation": "X", "provenance": "P"},   # XP: Read, Parallel     2: {"orientation": "X", "provenance": "U"},   # XU: Read, User-driven     3: {"orientation": "X", "provenance": "S"},   # XS: Read, System-guided     4: {"orientation": "Y", "provenance": "C"},   # YC: Index, Canon     5: {"orientation": "Y", "provenance": "P"},   # YP: Index, Parallel     6: {"orientation": "Y", "provenance": "U"},   # YU: Index, User memory     7: {"orientation": "Y", "provenance": "S"},   # YS: Index, System memory     8: {"orientation": "A", "provenance": "C"},   # AC: Ask, Canon constraints     9: {"orientation": "A", "provenance": "P"},   # AP: Ask, Parallel constraints     10: {"orientation": "A", "provenance": "U"},  # AU: Ask, User intent     11: {"orientation": "A", "provenance": "S"},  # AS: Ask, System plan     12: {"orientation": "Z", "provenance": "C"},  # ZC: Receive, Canon-bounded gen     13: {"orientation": "Z", "provenance": "P"},  # ZP: Receive, Parallel gen     14: {"orientation": "Z", "provenance": "U"},  # ZU: Receive, User-prompted gen     15: {"orientation": "Z", "provenance": "S"}   # ZS: Receive, System-prompted gen } def encode_symbol(char_hex: int, behavior: int) -> int:     """Combine character and behavior into one-byte symbol code."""     return ((char_hex & 0xF) << 4) | (behavior & 0xF) def decode_symbol(symbol_code: int) -> (int, int):     """Split symbol code into (char_hex, behavior)."""     return (symbol_code >> 4) & 0xF, symbol_code & 0xF  # qdpi/page_schema.py – Pydantic model for page frontmatter (YAML schema) from pydantic import BaseModel, Field from typing import List, Optional, Dict, Any class PageSchema(BaseModel):     page_id: str     symbol_code: int     character: str     orientation: str     provenance: str     trajectory: str     voice_preset: Optional[str]     inputs: Dict[str, Any] = Field(default_factory=dict)      # e.g. seeds, motifs, constraints     text: str     edges: Dict[str, Any] = Field(default_factory=dict)       # next_within, loop_back_to, forks     validation: Dict[str, Any] = Field(default_factory=dict)  # style_ok, continuity_score, etc.  # qdpi/policy.py – Simple gating policy MAX_RETURN_DEBT = 2 MAX_BRANCH_DEPTH = 3 def allow_fork(section: int, open_branches: int, return_debt: int) -> bool:     """Decide if we can start a new branch in this section."""     if return_debt > MAX_RETURN_DEBT:         return False  # too much return debt, disallow fork     # (We might also limit total open branches per section)     return open_branches < 3  # allow up to 2 open branches at a time, for example def require_return(branch_length: int) -> bool:     """Check if branch must return now (TTL reached)."""     return branch_length >= MAX_BRANCH_DEPTH  # qdpi/index.py – Stub index functions def extract_entities(text: str) -> List[str]:     # TODO: actual NER; for now, return empty or simple heuristic     return [] def extract_motifs(text: str) -> List[str]:     # TODO: domain-specific motif extraction; stub     return [] def find_unresolved_anchors(text: str) -> List[str]:     # If there are any placeholders or unanswered questions, list them     return [] def compute_novelty(text: str) -> float:     return 0.0  # stub: 0.0 = no novelty (all canon), 1.0 = very novel def compute_coherence(text: str) -> float:     return 1.0  # stub: 1.0 = fully coherent async def embed_text(text: str) -> bytes:     # Use vector_service or openai to get embedding; return as bytes     from backend.app.vector_service import get_vector_service     vs = get_vector_service()     try:         res = await vs.embed_text(text, content_id="temp", content_type="page")         # Convert list of floats to bytes (for BLOB storage) – e.g. using numpy         import numpy as np         vec = np.array(res.embedding, dtype=np.float32)         return vec.tobytes()     except Exception as e:         return b''  # DSPy Signatures (in qdpi/pipeline_signatures.py, for example) import dspy class ReadSignature(dspy.Signature):     symbol_code: int     # outputs:     render: str     summary: str     edges: dict     context: dict class IndexSignature(dspy.Signature):     text: str     # outputs:     entities: list     motifs: list     anchors: list     scores: dict                # e.g., {"novelty": float, "coherence": float} class AskSignature(dspy.Signature):     state: dict     user_intent: str = ""     # outputs (a "plan"):     trajectory: str     voice: dict     constraints: dict     sources: list class ReceiveSignature(dspy.Signature):     plan: dict     # outputs (new page content and metadata):     page_id: str     page_text: str     edges: dict     validation: dict  # QDPIProgram definition (qdpi/program.py) class QDPIProgram(dspy.Module):     def __init__(self):         # Prediction or ChainOfThought components for each stage         self.read = dspy.Predict(ReadSignature)         self.index = dspy.Predict(IndexSignature)         self.ask = dspy.Predict(AskSignature)         self.receive = dspy.Predict(ReceiveSignature)         self.select_traj = dspy.Select(options=["T0","T1","T2","T3"])     def forward(self, symbol_code: int, user_intent: str = ""):         # Step X: Read         x = self.read(symbol_code)         # Step Y: Index         y = self.index(x.render if x.render else "")         # Step A: Ask/Plan         plan = self.ask(state={"x": x, "y": y}, user_intent=user_intent)         # Decide trajectory based on policy (stub logic for now):         chosen_T = "T0"         if user_intent:             chosen_T = "T2" if "fork" in user_intent.lower() else "T3" if "drift" in user_intent.lower() else "T0"         # If unresolved anchors, enforce a loop-back         if y.anchors:             chosen_T = "T1"         plan.trajectory = self.select_traj(context={"intent": user_intent, "scores": y.scores}) or chosen_T         # Step Z: Receive (generate new page)         z = self.receive(plan)         # Post-generation Index (re-index the generated text)         _signals = self.index(z.page_text)         return z  # FastAPI pipeline endpoints (app/api/pipeline.py) from fastapi import APIRouter, Depends, HTTPException from backend.app.database import get_database  # to interact with DB router = APIRouter(prefix="/api/v1/pipeline", tags=["pipeline"]) # Assume we have a globally initialized QDPIProgram instance and LLMWrapper from qdpi.program import QDPIProgram qdpi_program = QDPIProgram() from backend.app.llm_wrapper import LLMWrapper llm = LLMWrapper()  @router.post("/run/symbol/{symbol_code}") async def run_symbol(symbol_code: str, body: Dict[str, str] = None, db = Depends(get_database)):     """Run QDPI pipeline for a given symbol code (hex or int)."""     user_intent = ""     if body and "user_intent" in body:         user_intent = body["user_intent"]     # Parse symbol_code (e.g. "0x1A" or "26")     try:         code_val = int(symbol_code, 0)  # auto-detect hex (0x) or decimal     except ValueError:         raise HTTPException(400, detail="Invalid symbol code format.")     # Execute pipeline result = qdpi_program(code_val, user_intent=user_intent)     # Persist the new page if any was generated     if result.page_id:         new_page = {             "id": result.page_id,             "symbol_id": CHARACTER_MAP[(code_val >> 4) & 0xF],             "rotation": 0,  # could infer from behavior if needed             "page_type": "AI_RESPONSE" if BEHAVIOR_MAP[code_val & 0xF]["orientation"] == "Z" else "PRIMARY",             "parent_id": None,             "prompt_type": None,             "text": result.page_text,             "author": "AI",             "section": None,  # fill if known             "canonical": False         }         try:             await db.create_page(new_page)         except Exception as e:             raise HTTPException(500, detail=f"Failed to save page: {e}")         # Save edges         for etype, tgt in result.edges.items():             if tgt:                 await db.create_edge(result.page_id, etype.upper(), tgt)  # assuming such a method exists or we use Stargate directly     return {"page_id": result.page_id, "text": result.page_text, "edges": result.edges, "trajectory": result.plan.trajectory}  @router.get("/stats/section/{section_id}") async def get_section_stats(section_id: int, db = Depends(get_database)):     """Get branching stats for a section (open branches, return debt)."""     stats = {"open_branches": 0, "return_debt": 0}     try:         row = await db.get_section_counters(section_id)         if row:             stats["open_branches"] = row.open_branches             stats["return_debt"] = row.return_debt     except Exception as e:         raise HTTPException(500, detail=f"Error fetching stats: {e}")     return {"section": section_id, **stats}`

_(The above code is illustrative and may need adaptation to fit exactly into the repository’s structure. We’ve used synchronous calls for simplicity in parts; in practice, `qdpi_program.forward` might be async if DSPy requires. Also, database operations via `db.create_edge` or `db.get_section_counters` would need to be added to the `CassandraDatabase` class or handled via CQL in this new code.)_

## **Test Plan (PyTest Outline)**

We will create a test module `test_qdpi_pipeline.py` with the following scenarios:

`import pytest from qdpi import symbols, policy, index from qdpi.program import QDPIProgram  def test_encode_decode_all():     for code in range(256):         char, beh = symbols.decode_symbol(code)         recombined = symbols.encode_symbol(char, beh)         assert recombined == code, f"Round-trip failed for code {code:#02x}"  def test_behavior_mapping():     # Spot-check a few known mappings     assert symbols.BEHAVIOR_MAP[0]["orientation"] == "X" and symbols.BEHAVIOR_MAP[0]["provenance"] == "C"     assert symbols.BEHAVIOR_MAP[10]["orientation"] == "A" and symbols.BEHAVIOR_MAP[10]["provenance"] == "U"  def test_policy_fork_and_return():     # When return_debt is high, no forks allowed     assert policy.allow_fork(section=1, open_branches=0, return_debt=5) is False     # Under threshold, allow if open_branches is small     assert policy.allow_fork(section=1, open_branches=1, return_debt=0) is True     # Branch TTL enforcement     assert policy.require_return(branch_length=3) is True     assert policy.require_return(branch_length=1) is False  @pytest.mark.asyncio async def test_pipeline_continuation(monkeypatch):     """Test a simple T0 continuation from a canon page."""     prog = QDPIProgram()     # Monkeypatch LLM generation to avoid external calls     async def dummy_generate_response(prompt, system=None, preferred_backend=None):         return type("Resp", (), {"text": "Continuation text.", "model_used": "dummy", "backend": "dummy", "generation_time_ms": 0})     from backend.app import llm_wrapper     monkeypatch.setattr(llm_wrapper.LLMWrapper, "generate_response", dummy_generate_response)     # Assume symbol 0x00 (an author XC) corresponds to an existing page with text     # Monkeypatch Read to return a dummy page content     async def dummy_read(symbol_code):         return type("X", (), {"render": "Canon page text.", "summary": "", "edges": {}, "context": {}})     monkeypatch.setattr(prog, "read", lambda code: dummy_read(code))     result = prog.forward(0x00, user_intent="")     # The result should have page_text from dummy_generate_response     assert "Continuation text." in result.page_text     assert result.edges.get("next_within") or result.edges.get("loop_back_to") is not None  @pytest.mark.asyncio async def test_pipeline_branch(monkeypatch):     """Test that branching sets return_to and updates counters properly."""     prog = QDPIProgram()     # Patch read to provide some anchors forcing a branch     async def dummy_read(symbol_code):         return type("X", (), {"render": "Text with unresolved mystery [ANCHOR].", "summary": "", "edges": {}, "context": {}})     monkeypatch.setattr(prog, "read", lambda code: dummy_read(code))     # Patch index to detect anchor     monkeypatch.setattr(index, "find_unresolved_anchors", lambda text: ["[ANCHOR]"])     # Patch LLM to give a branch outcome text     async def dummy_gen(prompt, system=None, preferred_backend=None):         return type("Resp", (), {"text": "Branch text that will return later.", "model_used": "dummy", "backend": "dummy", "generation_time_ms": 0})     monkeypatch.setattr(llm_wrapper.LLMWrapper, "generate_response", dummy_gen)     result = prog.forward(0x0E, user_intent="fork")  # Suppose 0x0E corresponds to a ZU (User-prompted gen)     # Verify that trajectory was set to T1 (because anchor present) or T2 (because user said fork)     assert result.plan.trajectory in ("T1", "T2")     # If branch, ensure return_to is set in edges or plan     if result.plan.trajectory.startswith("T2"):         assert result.edges.get("loop_back_to") is not None or result.plan.return_to is not None`

_(These are high-level test ideas – actual test code may differ. We assume monkeypatching to isolate LLM calls and possibly DB calls. We will also create fixtures for a temporary keyspace or use the mock DB for tests of DB-dependent functionality.)_

## **Runbook (Dev Manual Steps)**

Once implementation is done, here’s how to run and verify the new pipeline:

1. **Setup environment:** Ensure `.env` is configured for Cassandra. For example:
    
    `export DATABASE_URL=cassandra://localhost:9042/gibsey export CASSANDRA_KEYSPACE=gibsey export STARGATE_URL=http://localhost:8082   # if using Stargate export OPENAI_API_KEY=<your-key>   # if using OpenAI for embeddings/LLM (optional) export OLLAMA_BASE_URL=http://localhost:11434`
    
    Start Cassandra (and Stargate if needed). If using Docker, a `docker-compose.yml` may be provided – if so, run `docker-compose up cassandra stargate`.
    
2. **Apply schema:** Run the migration or CQL to create tables:
    
    `cqlsh -f scripts/cassandra_init.cql # OR run the Python migration script python backend/scripts/init_db.py   # this will wait for Cassandra, then call setup_optimized_schema etc.`
    
    Verify in cqlsh that the `pages` table and others exist (`DESCRIBE TABLES` in the keyspace).
    
3. **Ingest canonical story:** Execute the ingestion script to load initial content:
    
    `python scripts/ingest_canon.py gibsey-canon/corpus/pages/`
    
    (The script might autodiscover the corpus directory. If using `texts.json`, it could be `python scripts/ingest_canon.py --from-json src/assets/texts.json`). This should insert 710 rows into `pages` and corresponding `NEXT` edges in `page_edges`. On completion, check a few pages via CQL or API:
    
    `cqlsh> SELECT page_id, text, symbol_code FROM pages LIMIT 5;`
    
    Confirm you see entries like "S01-P0001" etc. Also ensure edges:
    
    `cqlsh> SELECT * FROM page_edges WHERE page_id='S01-P0001';`
    
    should show a NEXT edge to S01-P0002.
    
4. **Run the API server:** Launch the FastAPI app (with Uvicorn or Hypercorn):
    
    `uvicorn backend.main:app --reload`
    
    Check `http://localhost:8000/api/docs` – the documentation should now include our new endpoints under “pipeline”.
    
5. **Test a pipeline run (basic):** Using curl or a REST client, try:
    
    `curl -X POST "http://localhost:8000/api/v1/pipeline/run/symbol/0x00"`
    
    This should trigger the pipeline for symbol 0x00 (an author, XC). The response JSON should contain at least a `page_id` (likely "S01-P0002" if it continued canon) and some text. If `user_intent` is desired, include a JSON body:
    
    `curl -X POST "http://localhost:8000/api/v1/pipeline/run/symbol/0x0A" \      -H "Content-Type: application/json" \      -d '{"user_intent": "Tell me more about this side story."}'`
    
    Here 0x0A is AU (Ask with user intent) for an author – adjust as needed. The output should be a newly generated page (likely with a random UUID page_id since it’s off-canon) and trajectory perhaps T2 or T3.
    
6. **Inspect results:** Take the returned `page_id` and call:
    
    `curl "http://localhost:8000/api/v1/page/<PAGE_ID>"`
    
    (or `/pages/<PAGE_ID>` if we use the existing endpoint) to get the stored page. Confirm the text matches and edges are present (for canon, `next_within`; for a branch, `loop_back_to` etc.). You can also query the `section_counters` via:
    
    `curl "http://localhost:8000/api/v1/pipeline/stats/section/1"`
    
    to see open_branches and return_debt for section 1 (should be 0 if none).
    
7. **Run tests:** Execute `pytest` to run the new test suite. All tests in `test_qdpi_pipeline.py` should pass. Pay attention to any failing assertion – e.g., if encode/decode fails or if a pipeline integration test hangs (which might indicate an await issue or missing monkeypatch on LLM calls).
    
8. **Monitor logs:** The FastAPI console will log each pipeline step (especially if we add logging inside each Signature or after each stage). Ensure there are no exceptions. If something fails mid-pipeline, the API should return an error; check the logs to identify the stage. Common fixes: database connectivity issues (make sure keyspace names match), or type errors in DSPy usage (we might adjust the forward calls to use `await` if needed, e.g. `result = await qdpi_program(symbol_int, ...)` if `QDPIProgram.forward` is async).
    

By following these steps, you should be able to stand up the QDPI pipeline and generate story content on demand, using the 256-symbol addressing scheme.

## Definition of Done & Acceptance Criteria

The implementation is complete when we can: (a) ingest all canonical pages into the system; (b) generate new pages via the API that logically integrate into the story graph; and (c) enforce basic narrative rules (no infinite branches, branches eventually rejoin canon). All new code should be minimal yet functional, and all tests should pass. We should see that running the pipeline yields a new page with a valid ID, the page is stored, edges connect correctly (and can be navigated by the frontend), and the system respects any explicit user prompt provided.

In summary, **Done** is: _“A user can start at Preface (S01-P0007), invoke the pipeline to continue the story or ask a question, get a new piece of narrative generated, and the system properly records this new page and how it links to the rest of the book. The process is repeatable for different symbols/intents, without manual intervention, and with clear logs and metrics for branch management.”_ All known risks are documented with mitigation strategies (as above), and any out-of-scope features (like Kafka integration or advanced semantic indexing) are gracefully stubbed or deferred without breaking the core flow.